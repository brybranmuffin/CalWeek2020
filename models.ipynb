{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ecbe5ee5",
      "metadata": {
        "id": "ecbe5ee5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU Acceleration --IGNORE IF NOT USING COLAB--"
      ],
      "metadata": {
        "id": "37WpkwVyaTpj"
      },
      "id": "37WpkwVyaTpj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Below line allows for GPU acceleration of sci-kit modeling apis\n",
        "%load_ext cuml.accel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFJYpBxHW48f",
        "outputId": "b2b2301b-94a3-48bd-8acf-e48d5319588f"
      },
      "id": "oFJYpBxHW48f",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuML: Accelerator installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3bcfb34",
        "outputId": "ac30ab1b-db18-4c01-8ff6-36f814f5742f"
      },
      "source": [
        "#checking cuml and IPython versions, some versions are not compatible. cuml 25.06.00 and IPython 7.34.0 are compatible\n",
        "\n",
        "import IPython\n",
        "print(f\"IPython version: {IPython.__version__}\")\n",
        "\n",
        "try:\n",
        "    import cuml\n",
        "    print(f\"cuml version: {cuml.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"cuml is not installed or could not be imported.\")"
      ],
      "id": "d3bcfb34",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IPython version: 7.34.0\n",
            "cuml version: 25.06.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If you are mounting the files from drive"
      ],
      "metadata": {
        "id": "Kc-5gTTNapW1"
      },
      "id": "Kc-5gTTNapW1"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPuEcFBale6Z",
        "outputId": "a3b33425-351d-41e9-e60b-cbe001239856"
      },
      "id": "cPuEcFBale6Z",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using CSV"
      ],
      "metadata": {
        "id": "HNv08VPZaxM5"
      },
      "id": "HNv08VPZaxM5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "658d6177",
      "metadata": {
        "id": "658d6177"
      },
      "outputs": [],
      "source": [
        "main_combined_df_path = '/content/drive/MyDrive/final combined data/all_features_df.csv'\n",
        "# main_combined_df_path = 'all_features_df.csv'\n",
        "df = pd.read_csv(main_combined_df_path)\n",
        "df['claim_text'] = df['claim_text'].apply(ast.literal_eval)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Pickle File --RECOMMENDED--"
      ],
      "metadata": {
        "id": "g4U_-GaJa0m8"
      },
      "id": "g4U_-GaJa0m8"
    },
    {
      "cell_type": "code",
      "source": [
        "pkl_path = ''\n",
        "df = pd.read_pickle()"
      ],
      "metadata": {
        "id": "fib_PHFAZOfd"
      },
      "id": "fib_PHFAZOfd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2580ff53",
      "metadata": {
        "id": "2580ff53"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "print(df.columns)\n",
        "print(len(df))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Vectorizer Creation --IGNORE--"
      ],
      "metadata": {
        "id": "qhcxLPBnoAHp"
      },
      "id": "qhcxLPBnoAHp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorization does work if you do it from scratch, however it does not save to the joblib file properly for some reason. This is not the preferred way to make a sparse matrix since Karl has provided a vectorizationn object that does work with claims text"
      ],
      "metadata": {
        "id": "Ii0KLPiochk5"
      },
      "id": "Ii0KLPiochk5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f32687",
      "metadata": {
        "id": "42f32687"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# tfidf_features = joblib.load('./tfidf_features.joblib')\n",
        "# tlidf_vectorizer = joblib.load('./tfidf_vectorizer.joblib')\n",
        "\n",
        "# suppose texts is a NumPy array (or list) of token lists\n",
        "# e.g. [['novel', 'catalyst'], ['apparatus', 'for', '...'], ...]\n",
        "\n",
        "def identity(tokens):\n",
        "    return tokens\n",
        "\n",
        "tlidf_vectorizer = TfidfVectorizer(\n",
        "    analyzer=identity,     # accept pre-tokenized lists\n",
        "    lowercase=False,       # tokens already normalized\n",
        "    dtype=float,\n",
        "    norm=\"l2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db5be850",
      "metadata": {
        "id": "db5be850",
        "outputId": "675be226-f6bb-4bb7-b628-af7a2ade36f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating vocabulary...\n",
            "Vocabulary created with 637,906 unique tokens\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:2043: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. <class 'float'> 'dtype' will be converted to np.float64.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def build_vocabulary(input_df):\n",
        "    #This builds a vocabulary based on the tokens in the claim_text column\n",
        "    #it saves the vocabulary to whatever you put in output_vocab_file which should be a .json file\n",
        "\n",
        "    vocab = set()\n",
        "\n",
        "    reader = input_df['claim_text']\n",
        "    print('Creating vocabulary...')\n",
        "    for token_list in reader:\n",
        "        vocab.update(token_list)\n",
        "    print(f\"Vocabulary created with {len(vocab):,} unique tokens\")\n",
        "\n",
        "    # Convert set to sorted list for stable saving\n",
        "    vocab_list = sorted(vocab)\n",
        "\n",
        "    return vocab_list\n",
        "\n",
        "def build_claim_vectorizer(vocab):\n",
        "    \"\"\"Return a TF-IDF vectorizer with the same settings as fit_tfidf_vectorizer.\"\"\"\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        vocabulary={tok: i for i, tok in enumerate(vocab)},\n",
        "        analyzer=lambda tokens: tokens,   # accept pre-tokenized lists\n",
        "        lowercase=False,\n",
        "        norm=\"l2\",\n",
        "        dtype=float,\n",
        "        use_idf=True,\n",
        "\n",
        "    )\n",
        "    return vectorizer\n",
        "\n",
        "# usage\n",
        "# df['claim_text'] is a column of lists, e.g. ['apparatus', 'comprising', ...]\n",
        "\n",
        "vocab_list = build_vocabulary(df)\n",
        "vectorizer = build_claim_vectorizer(vocab_list)\n",
        "sparse_matrix = vectorizer.fit_transform(df['claim_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982a04b9",
      "metadata": {
        "id": "982a04b9",
        "outputId": "f3dc8762-ddd8-46a2-ce46-557d75a8a2f1"
      },
      "outputs": [
        {
          "ename": "PicklingError",
          "evalue": "Can't pickle <function build_claim_vectorizer.<locals>.<lambda> at 0x9ac9c4fe0>: it's not found as __main__.build_claim_vectorizer.<locals>.<lambda>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m vectorizer\u001b[38;5;241m.\u001b[39midf_ \u001b[38;5;241m=\u001b[39m idf\n\u001b[1;32m     15\u001b[0m vectorizer\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39m_idf_diag \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mspdiags(idf, diags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(idf), n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(idf))\n\u001b[0;32m---> 16\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(vectorizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tfidf_vectorizer_v2.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/site-packages/joblib/numpy_pickle.py:553\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m     NumpyPickler(filename, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/pickle.py:484\u001b[0m, in \u001b[0;36m_Pickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mstart_framing()\n\u001b[0;32m--> 484\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(obj)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(STOP)\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframer\u001b[38;5;241m.\u001b[39mend_framing()\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/site-packages/joblib/numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/pickle.py:601\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTuple returned by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    598\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo to six elements\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m reduce)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# Save the reduce() output and finally memoize the object\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_reduce(obj\u001b[38;5;241m=\u001b[39mobj, \u001b[38;5;241m*\u001b[39mrv)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/pickle.py:715\u001b[0m, in \u001b[0;36m_Pickler.save_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, state_setter, obj)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state_setter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 715\u001b[0m         save(state)\n\u001b[1;32m    716\u001b[0m         write(BUILD)\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;66;03m# If a state_setter is specified, call it instead of load_build\u001b[39;00m\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;66;03m# to update obj's with its previous state.\u001b[39;00m\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;66;03m# First, push state_setter and its tuple of expected arguments\u001b[39;00m\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;66;03m# (obj, state) onto the stack.\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/site-packages/joblib/numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/pickle.py:558\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    556\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 558\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/pickle.py:990\u001b[0m, in \u001b[0;36m_Pickler.save_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite(MARK \u001b[38;5;241m+\u001b[39m DICT)\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemoize(obj)\n\u001b[0;32m--> 990\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_setitems(obj\u001b[38;5;241m.\u001b[39mitems())\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/pickle.py:1014\u001b[0m, in \u001b[0;36m_Pickler._batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tmp:\n\u001b[1;32m   1013\u001b[0m         save(k)\n\u001b[0;32m-> 1014\u001b[0m         save(v)\n\u001b[1;32m   1015\u001b[0m     write(SETITEMS)\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n:\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/site-packages/joblib/numpy_pickle.py:355\u001b[0m, in \u001b[0;36mNumpyPickler.save\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    352\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mwrite_array(obj, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m, obj)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/pickle.py:558\u001b[0m, in \u001b[0;36m_Pickler.save\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    556\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mget(t)\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 558\u001b[0m     f(\u001b[38;5;28mself\u001b[39m, obj)  \u001b[38;5;66;03m# Call unbound method with explicit self\u001b[39;00m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# Check private dispatch table if any, or else\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# copyreg.dispatch_table\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.13/pickle.py:1087\u001b[0m, in \u001b[0;36m_Pickler.save_global\u001b[0;34m(self, obj, name)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     obj2, parent \u001b[38;5;241m=\u001b[39m _getattribute(module, name)\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m):\n\u001b[0;32m-> 1087\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PicklingError(\n\u001b[1;32m   1088\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pickle \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m: it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not found as \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1089\u001b[0m         (obj, module_name, name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1091\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj:\n",
            "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function build_claim_vectorizer.<locals>.<lambda> at 0x9ac9c4fe0>: it's not found as __main__.build_claim_vectorizer.<locals>.<lambda>"
          ]
        }
      ],
      "source": [
        "from scipy import sparse\n",
        "num_docs = len(df['claim_text'])\n",
        "token_to_idx = vectorizer.vocabulary_\n",
        "df_counts = np.zeros(len(token_to_idx), dtype=np.int64)\n",
        "\n",
        "for tokens in df['claim_text']:\n",
        "    for tok in set(tokens):\n",
        "        idx = token_to_idx.get(tok)\n",
        "        if idx is not None:\n",
        "            df_counts[idx] += 1\n",
        "\n",
        "\n",
        "idf = np.log((1 + num_docs) / (1 + df_counts)) + 1.0\n",
        "vectorizer.idf_ = idf\n",
        "vectorizer._tfidf._idf_diag = sparse.spdiags(idf, diags=0, m=len(idf), n=len(idf))\n",
        "joblib.dump(vectorizer, './tfidf_vectorizer_v2.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorizer Loading -- DONT IGNORE --"
      ],
      "metadata": {
        "id": "WXbA3SCMoIPs"
      },
      "id": "WXbA3SCMoIPs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c1ed60",
      "metadata": {
        "id": "c4c1ed60",
        "outputId": "0bb5d2de-eb2e-47bd-cf4a-142e97383766"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:2043: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. <class 'float'> 'dtype' will be converted to np.float64.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# sparse_matrix = tlidf_vectorizer.transform(df['claim_text'])\n",
        "tlidf_vectorizer = joblib.load('/content/drive/MyDrive/final combined data/tfidf_vectorizer_for_bryant.joblib')\n",
        "sparse_matrix = tlidf_vectorizer.fit_transform(df['claim_text'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVD of tfidf matrix, Creation of X data and Y data for train-test-split"
      ],
      "metadata": {
        "id": "0_eEY_godKTs"
      },
      "id": "0_eEY_godKTs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "186497d9",
      "metadata": {
        "id": "186497d9",
        "outputId": "929bfbae-0545-4fbc-c194-a3ccd2eecf33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3909923, 637906)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "num_dimensions = 100\n",
        "svd = TruncatedSVD(\n",
        "    n_components=num_dimensions,\n",
        "    n_iter=7,\n",
        "    random_state=70\n",
        ")\n",
        "sparse_matrix.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7187a41",
      "metadata": {
        "id": "a7187a41",
        "outputId": "164556a5-3c3a-4fab-800a-ca2b1269093b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing TruncatedSVD with 100 components...\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "extra_np = (\n",
        "    df.drop(columns=['claim_text', 'application_number', 'y_approved', 'y_101', 'y_102', 'y_103', 'y_112'])\n",
        "    .apply(pd.to_numeric, errors=\"coerce\")  # convert strings/bools -> numbers\n",
        "    .fillna(0)                              # replace NaNs\n",
        "    .to_numpy(dtype=np.float64)             # ensure a numeric dtype\n",
        ")\n",
        "\n",
        "extra_sparse = csr_matrix(extra_np)\n",
        "print(f\"Performing TruncatedSVD with {num_dimensions} components...\")\n",
        "tfidf_svd = svd.fit_transform(sparse_matrix)\n",
        "\n",
        "X_aug_combined = hstack([tfidf_svd, extra_sparse])\n",
        "X_aug_no_tfidf = extra_sparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9803b375",
      "metadata": {
        "id": "9803b375"
      },
      "outputs": [],
      "source": [
        "y_rejected = df['y_approved'] == 0\n",
        "y_rejected = y_rejected.astype(int)\n",
        "y_has_rej101 = df['y_101']\n",
        "y_has_rej102 = df['y_102']\n",
        "y_has_rej103 = df['y_103']\n",
        "y_has_rej112 = df['y_112']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45737e1e",
      "metadata": {
        "id": "45737e1e"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train test split of all data"
      ],
      "metadata": {
        "id": "FRiDi6fMdY9X"
      },
      "id": "FRiDi6fMdY9X"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is train test split of data without tfidf sparse matrix"
      ],
      "metadata": {
        "id": "QYfCpTfcddtX"
      },
      "id": "QYfCpTfcddtX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1352626a",
      "metadata": {
        "id": "1352626a",
        "outputId": "3d2cd212-4e87-4df5-8814-fe069a169e22"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_aug_no_tfidf_svd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#  getting train and test data for data with no tfidf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 3\u001b[0m X_train_rejected_no_tfidf, X_test_rejected_no_tfidf, y_train_rejected_no_tfidf, y_test_rejected_no_tfidf \u001b[38;5;241m=\u001b[39m train_test_split(X_aug_no_tfidf_svd, y_rejected, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m X_train_has_rej101_no_tfidf, X_test_has_rej101_no_tfidf, y_train_has_rej101_no_tfidf, y_test_has_rej101_no_tfidf \u001b[38;5;241m=\u001b[39m train_test_split(X_aug_no_tfidf_svd, y_has_rej101, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      5\u001b[0m X_train_has_rej102_no_tfidf, X_test_has_rej102_no_tfidf, y_train_has_rej102_no_tfidf, y_test_has_rej102_no_tfidf \u001b[38;5;241m=\u001b[39m train_test_split(X_aug_no_tfidf_svd, y_has_rej102, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_aug_no_tfidf_svd' is not defined"
          ]
        }
      ],
      "source": [
        "#  getting train and test data for data with no tfidf\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_rejected_no_tfidf, X_test_rejected_no_tfidf, y_train_rejected_no_tfidf, y_test_rejected_no_tfidf = train_test_split(X_aug_no_tfidf_svd, y_rejected, test_size=0.2, random_state=42)\n",
        "X_train_has_rej101_no_tfidf, X_test_has_rej101_no_tfidf, y_train_has_rej101_no_tfidf, y_test_has_rej101_no_tfidf = train_test_split(X_aug_no_tfidf_svd, y_has_rej101, test_size=0.2, random_state=42)\n",
        "X_train_has_rej102_no_tfidf, X_test_has_rej102_no_tfidf, y_train_has_rej102_no_tfidf, y_test_has_rej102_no_tfidf = train_test_split(X_aug_no_tfidf_svd, y_has_rej102, test_size=0.2, random_state=42)\n",
        "X_train_has_rej103_no_tfidf, X_test_has_rej103_no_tfidf, y_train_has_rej103_no_tfidf, y_test_has_rej103_no_tfidf = train_test_split(X_aug_no_tfidf_svd, y_has_rej103, test_size=0.2, random_state=42)\n",
        "X_train_has_rej112_no_tfidf, X_test_has_rej112_no_tfidf, y_train_has_rej112_no_tfidf, y_test_has_rej112_no_tfidf = train_test_split(X_aug_no_tfidf_svd, y_has_rej112, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "090d1288",
      "metadata": {
        "id": "090d1288"
      },
      "outputs": [],
      "source": [
        "#  getting train and test data for data with tfidf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_rejected, X_test_rejected, y_train_rejected, y_test_rejected = train_test_split(X_aug_combined_svd, y_rejected, test_size=0.2, random_state=42)\n",
        "X_train_has_rej101, X_test_has_rej101, y_train_has_rej101, y_test_has_rej101 = train_test_split(X_aug_combined_svd, y_has_rej101, test_size=0.2, random_state=42)\n",
        "X_train_has_rej102, X_test_has_rej102, y_train_has_rej102, y_test_has_rej102 = train_test_split(X_aug_combined_svd, y_has_rej102, test_size=0.2, random_state=42)\n",
        "X_train_has_rej103, X_test_has_rej103, y_train_has_rej103, y_test_has_rej103 = train_test_split(X_aug_combined_svd, y_has_rej103, test_size=0.2, random_state=42)\n",
        "X_train_has_rej112, X_test_has_rej112, y_train_has_rej112, y_test_has_rej112 = train_test_split(X_aug_combined_svd, y_has_rej112, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modeling"
      ],
      "metadata": {
        "id": "vdSmLvX1donA"
      },
      "id": "vdSmLvX1donA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa4919c",
      "metadata": {
        "id": "caa4919c"
      },
      "outputs": [],
      "source": [
        "def get_best_random_gb_params(X_train, y_train, scoring = 'f1', n_iter = 20):\n",
        "    gb = GradientBoostingClassifier(random_state=0)\n",
        "    gb_space = {\n",
        "        \"n_estimators\": np.arange(50, 401),\n",
        "        \"learning_rate\": np.logspace(-3, 0, 100),\n",
        "        \"max_depth\": np.arange(1, 6),\n",
        "        \"min_samples_split\": np.arange(2, 21),\n",
        "    }\n",
        "    gb_rand = RandomizedSearchCV(\n",
        "        gb,\n",
        "        gb_space,\n",
        "        n_iter=n_iter,\n",
        "        scoring=scoring,\n",
        "        cv=5,\n",
        "        random_state=0,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    gb_rand.fit(X_train, y_train)\n",
        "    gb_best = gb_rand.best_params_\n",
        "    return gb_best\n",
        "\n",
        "def get_best_random_rf_params(X_train, y_train, scoring = 'f1', n_iter = 20):\n",
        "    rf = RandomForestClassifier(random_state=0)\n",
        "    rf_space = {\n",
        "        \"n_estimators\": np.arange(100, 801),\n",
        "        \"max_depth\": [None, *np.arange(5, 31)],\n",
        "        \"min_samples_split\": np.arange(2, 21),\n",
        "        \"min_samples_leaf\": np.arange(1, 11),\n",
        "        \"max_features\": [\"sqrt\", \"log2\", 0.5, None],\n",
        "    }\n",
        "    rf_rand = RandomizedSearchCV(\n",
        "        rf,\n",
        "        rf_space,\n",
        "        n_iter=n_iter,\n",
        "        scoring=scoring,\n",
        "        cv=5,\n",
        "        random_state=0,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    rf_rand.fit(X_train, y_train)\n",
        "    rf_best = rf_rand.best_params_\n",
        "    return rf_best\n",
        "\n",
        "def get_best_grid_search_gb_model(X_train, y_train, gb_best, scoring = 'f1'):\n",
        "    gb_grid = {\n",
        "        \"n_estimators\": [gb_best[\"n_estimators\"] - 50,\n",
        "                        gb_best[\"n_estimators\"],\n",
        "                        gb_best[\"n_estimators\"] + 50],\n",
        "        \"learning_rate\": [gb_best[\"learning_rate\"] * f for f in [0.5, 1.0, 1.5]],\n",
        "        \"max_depth\": [gb_best[\"max_depth\"]],\n",
        "        \"min_samples_split\": [gb_best[\"min_samples_split\"] - 1,\n",
        "                            gb_best[\"min_samples_split\"],\n",
        "                            gb_best[\"min_samples_split\"] + 1],\n",
        "    }\n",
        "    gb_grid = {k: [v] if not isinstance(v, list) else v for k, v in gb_grid.items()}\n",
        "    gb_grid = {k: [val for val in vals if val is not None and val > 0] for k, vals in gb_grid.items()}\n",
        "    gb_gridsearch = GridSearchCV(\n",
        "        GradientBoostingClassifier(random_state=0),\n",
        "        gb_grid,\n",
        "        scoring=scoring,\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    return gb_gridsearch\n",
        "\n",
        "def get_best_grid_search_rf_model(X_train, y_train, rf_best, scoring = 'f1'):\n",
        "    rf_grid = {\n",
        "        \"n_estimators\": [rf_best[\"n_estimators\"] - 100,\n",
        "                        rf_best[\"n_estimators\"],\n",
        "                        rf_best[\"n_estimators\"] + 100],\n",
        "        \"max_depth\": [None if rf_best[\"max_depth\"] is None else d for d in [\n",
        "            rf_best[\"max_depth\"] - 5,\n",
        "            rf_best[\"max_depth\"],\n",
        "            rf_best[\"max_depth\"] + 5,\n",
        "        ] if d is None or d > 0],\n",
        "        \"min_samples_split\": [max(2, rf_best[\"min_samples_split\"] - 1),\n",
        "                            rf_best[\"min_samples_split\"],\n",
        "                            rf_best[\"min_samples_split\"] + 1],\n",
        "        \"min_samples_leaf\": [max(1, rf_best[\"min_samples_leaf\"] - 1),\n",
        "                            rf_best[\"min_samples_leaf\"],\n",
        "                            rf_best[\"min_samples_leaf\"] + 1],\n",
        "        \"max_features\": [rf_best[\"max_features\"]],\n",
        "    }\n",
        "    rf_grid = {k: [v] if not isinstance(v, list) else v for k, v in rf_grid.items()}\n",
        "    rf_grid = {k: [val for val in vals if val is not None] for k, vals in rf_grid.items()}\n",
        "    rf_gridsearch = GridSearchCV(\n",
        "        RandomForestClassifier(random_state=0),\n",
        "        rf_grid,\n",
        "        scoring=scoring,\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "    return rf_gridsearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c9fe10",
      "metadata": {
        "id": "c0c9fe10"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import time\n",
        "\n",
        "def create_models(X_train, y_train, rf_params = None, gb_params = None):\n",
        "    models = {}\n",
        "    models['LogR'] = LogisticRegression(solver='saga')\n",
        "    models['LinR'] = LinearRegression()\n",
        "    if rf_params:\n",
        "        models['RF'] = RandomForestClassifier(**rf_params)\n",
        "    else:\n",
        "        models['RF'] = RandomForestClassifier(n_estimators=100)\n",
        "    if gb_params:\n",
        "        models['GB'] = GradientBoostingClassifier(**gb_params)\n",
        "    else:\n",
        "        models['GB'] = GradientBoostingClassifier(n_estimators=100)\n",
        "    print(\"Fitting Logistic Regression\")\n",
        "    start_time = time.time()\n",
        "    models['LogR'].fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"Logistic Regression training time: {end_time - start_time} seconds\")\n",
        "    print(\"Fitting Linear Regression\")\n",
        "    start_time = time.time()\n",
        "    models['LinR'].fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"Linear Regression training time: {end_time - start_time} seconds\")\n",
        "    print(\"Fitting Random Forest\")\n",
        "    start_time = time.time()\n",
        "    models['RF'].fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"Random Forest training time: {end_time - start_time} seconds\")\n",
        "    print(\"Fitting Gradient Boosting\")\n",
        "    start_time = time.time()\n",
        "    models['GB'].fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "    print(f\"Gradient Boosting training time: {end_time - start_time} seconds\")\n",
        "    return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39bc5ed9",
      "metadata": {
        "id": "39bc5ed9"
      },
      "outputs": [],
      "source": [
        "# Fill in below with the right data, currently filled in for overall rejections using X values with no tfidf data\n",
        "X_train = X_train_rejected_no_tfidf\n",
        "y_train = y_train_rejected_no_tfidf\n",
        "X_test = X_test_rejected_no_tfidf\n",
        "y_test = y_test_rejected_no_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "715c40a3",
      "metadata": {
        "id": "715c40a3"
      },
      "outputs": [],
      "source": [
        "rf_params_random = get_best_random_rf_params(X_train, y_train)\n",
        "gb_params_random = get_best_random_gb_params(X_train, y_train)\n",
        "rf_params_grid = get_best_grid_search_rf_model(X_train, y_train, rf_params_random)\n",
        "gb_params_grid = get_best_grid_search_gb_model(X_train, y_train, gb_params_random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3113d159",
      "metadata": {
        "id": "3113d159",
        "outputId": "b67d470f-f324-44d0-f307-f985bdb9e890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting Logistic Regression\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/lib/python3.13/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression training time: 276.5072569847107 seconds\n",
            "Fitting Linear Regression\n",
            "Linear Regression training time: 18.159186840057373 seconds\n",
            "Fitting Random Forest\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "#fill in rf params and gb params with the best params from randomized search or grid search\n",
        "rf_params = rf_params_grid\n",
        "gb_params = gb_params_grid\n",
        "# get accuracy, F1, precision, recall, and confusion matrix\n",
        "models = create_models(X_train, y_train)\n",
        "for model_name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "    print(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n",
        "    print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
        "    print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
        "    print(f\"Confusion Matrix: {confusion_matrix(y_test, y_pred)}\")\n",
        "    print(\"\\n\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}